{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1 Dataset Acquisition\n",
    "\n",
    "The dataset that I choose is about Liverpool F.C. from wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi #pip install wikipedia-api\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en for English\n",
    "wiki = wikipediaapi.Wikipedia('A2 (st124482@ait.asia)','en')\n",
    "# load Liverpool F.C. content\n",
    "page = wiki.page('Liverpool F.C.')\n",
    "# page.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liverpool Football Club is a professional football club based in Liverpool, England.  The club competes in the Premier League, the top tier of English football.  Founded in 1892, the club joined the Football League the following year and has played its home games at Anfield since its formation. Domestically, the club has won 19 league titles, eight FA Cups, a record nine League Cups and 16 FA Community Shields.  In international competitions, the club has won six European Cups, three UEFA Cups, four UEFA Super Cups—all English records—and one FIFA Club World Cup.  The club established itself as a major force in domestic and European football in the 1970s and 1980s, when Bill Shankly, Bob Paisley, Joe Fagan and Kenny Dalglish, led the club to a combined 11 League titles and four European Cups.  Liverpool won two further European Cups in 2005 and 2019 under the management of Rafael Benítez and Jürgen Klopp, respectively; the latter led Liverpool to a 19th league title in 2020, the club's first during the Premier League era.  Liverpool is one of the most valuable and widely supported clubs in the world.  The club has long-standing rivalries with Manchester United and Everton.  Under management by Shankly, in 1964 the team changed from red shirts and white shorts to an all-red home strip which has been used ever since.  The club's anthem is \"You'll Never Walk Alone\".  The club's supporters have been involved in two major tragedies.  The Heysel Stadium disaster, where escaping fans were pressed against a collapsing wall at the 1985 European Cup Final in Brussels, resulted in 39 deaths.  Most of these were Italians and Juventus fans.  Liverpool were given a six-year ban from European competition, and all other English clubs received a five-year ban.  The Hillsborough disaster in 1989, where 97 Liverpool supporters died in a crush against perimeter fencing, led to the elimination of fenced standing terraces in favour of all-seater stadiums in the top two tiers of English football.  Prolonged campaigning for justice saw further coroners inquests, commissions and independent panels that ultimately exonerated the fans.  History Liverpool F. C.  was founded following a dispute between the Everton committee and John Houlding, club president and owner of the land at Anfield.  After eight years at the stadium, Everton relocated to Goodison Park in 1892 and Houlding founded Liverpool F. C.  to play at Anfield.  Originally named \"Everton F. C.  and Athletic Grounds Ltd\" (Everton Athletic for short), the club became Liverpool F. C.  in March 1892 and gained official recognition three months later, after The Football Association refused to recognise the club as Everton. Liverpool played their first match on 1 September 1892, a pre-season friendly match against Rotherham Town, which they won 7–1.  The team Liverpool fielded against Rotherham was composed entirely of Scottish players—the players who came from Scotland to play in England in those days were known as the Scotch Professors.  Manager John McKenna had recruited the players after a scouting trip to Scotland—so they became known as the \"team of Macs\".  The team won the Lancashire League in its debut season and joined the Football League Second Division at the start of the 1893–94 season.  After the club was promoted to the First Division in 1896, Tom Watson was appointed manager.  He led Liverpool to its first league title in 1901, before winning it again in 1906. Liverpool reached its first FA Cup Final in 1914, losing 1–0 to Burnley.  It won consecutive League championships in 1922 and 1923, but did not win another trophy until the 1946–47 season, when the club won the First Division for a fifth time under the control of ex-West Ham United centre half George Kay.  Liverpool suffered its second Cup Final defeat in 1950, playing against Arsenal.  The club was relegated to the Second Division in the 1953–54 season.  Soon after Liverpool lost 2–1 to non-league Worcester City in the 1958–59 FA Cup, Bill Shankly was appointed manager.  Upon his arrival he released 24 players and converted a boot storage room at Anfield into a room where the coaches could discuss strategy; here, Shankly and other \"Boot Room\" members Joe Fagan, Reuben Bennett, and Bob Paisley began reshaping the team.  The club was promoted back into the First Division in 1962 and won it in 1964, for the first time in 17 years.  In 1965, the club won its first FA Cup.  In 1966, the club won the First Division but lost to Borussia Dortmund in the European Cup Winners' Cup final.  Liverpool won both the League and the UEFA Cup during the 1972–73 season, and the FA Cup again a year later.  Shankly retired soon afterwards and was replaced by his assistant, Bob Paisley.  In 1976, Paisley's second season as manager, the club won another League and UEFA Cup double.  The following season, the club retained the League title and won the European Cup for the first time, but it lost in the 1977 FA Cup Final.  Liverpool retained the European Cup in 1978 and regained the First Division title in 1979.  During Paisley's nine seasons as manager Liverpool won 20 trophies, including three European Cups, a UEFA Cup, six League titles and three consecutive League Cups; the only domestic trophy he did not win was the FA Cup.  Paisley retired in 1983 and was replaced by his assistant, Joe Fagan.  Liverpool won the League, League Cup and European Cup in Fagan's first season, becoming the first English side to win three trophies in a season.  Liverpool reached the European Cup final again in 1985, against Juventus at the Heysel Stadium.  Before kick-off, Liverpool fans breached a fence that separated the two groups of supporters and charged the Juventus fans.  The resulting weight of people caused a retaining wall to collapse, killing 39 fans, mostly Italians.  The incident became known as the Heysel Stadium disaster.  The match was played in spite of protests by both managers, and Liverpool lost 1–0 to Juventus.  As a result of the tragedy, English clubs were banned from participating in European competition for five years; Liverpool received a ten-year ban, which was later reduced to six years.  Fourteen Liverpool fans received convictions for involuntary manslaughter. Fagan had announced his retirement just before the disaster and Kenny Dalglish was appointed as player-manager.  During his tenure, the club won another three league titles and two FA Cups, including a League and Cup \"Double\" in the 1985–86 season.  Liverpool's success was overshadowed by the Hillsborough disaster: in an FA Cup semi-final against Nottingham Forest on 15 April 1989, hundreds of Liverpool fans were crushed against perimeter fencing.  Ninety-four fans died that day; the 95th victim died in hospital from his injuries four days later, the 96th died nearly four years later, without regaining consciousness, and the 97th, Andrew Devine, died of injuries sustained in the disaster in 2021.  After the Hillsborough disaster there was a government review of stadium safety.  The resulting Taylor Report paved the way for legislation that required top-division teams to have all-seater stadiums.  The report ruled that the main reason for the disaster was overcrowding due to a failure of police control.  Liverpool was involved in the closest finish to a league season during the 1988–89 season.  Liverpool finished equal with Arsenal on both points and goal difference, but lost the title on total goals scored when Arsenal scored the final goal in the last minute of the season. Dalglish cited the Hillsborough disaster and its repercussions as the reason for his resignation in 1991; he was replaced by former player Graeme Souness.  Under his leadership Liverpool won the 1992 FA Cup Final, but their league performances slumped, with two consecutive sixth-place finishes, eventually resulting in his dismissal in January 1994.  Souness was replaced by Roy Evans, and Liverpool went on to win the 1995 Football League Cup Final.  While they made some title challenges under Evans, third-place finishes in 1996 and 1998 were the best they could manage, and so Gérard Houllier was appointed co-manager in the 1998–99 season, and became the sole manager in November 1998 after Evans resigned.  In 2001, Houllier's second full season in charge, Liverpool won a \"treble\": the FA Cup, League Cup and UEFA Cup.  Houllier underwent major heart surgery during the 2001–02 season and Liverpool finished second in the League, behind Arsenal.  They won a further League Cup in 2003, but failed to mount a title challenge in the two seasons that followed.  Houllier was replaced by Rafael Benítez at the end of the 2003–04 season.  Despite finishing fifth in Benítez's first season, Liverpool won the 2004–05 UEFA Champions League, beating A. C.  Milan 3–2 in a penalty shootout after the match ended with a score of 3–3.  The following season, Liverpool finished third in the Premier League and won the 2006 FA Cup Final, beating West Ham United in a penalty shootout after the match finished 3–3.  American businessmen George Gillett and Tom Hicks became the owners of the club during the 2006–07 season, in a deal which valued the club and its outstanding debts at £218. 9 million.  The club reached the 2007 UEFA Champions League Final against Milan, as it had in 2005, but lost 2–1.  During the 2008–09 season Liverpool achieved 86 points, its then-highest Premier League points total, prior to the record-breaking 2018–19 season, and finished as runners up to Manchester United. In the 2009–10 season, Liverpool finished seventh in the Premier League and failed to qualify for the Champions League.  Benítez subsequently left by mutual consent and was replaced by Fulham manager Roy Hodgson.  At the start of the 2010–11 season Liverpool was on the verge of bankruptcy and the club's creditors asked the High Court to allow the sale of the club, overruling the wishes of Hicks and Gillett.  John W.  Henry, owner of the Boston Red Sox and of Fenway Sports Group, bid successfully for the club and took ownership in October 2010.  Poor results during the start of that season led to Hodgson leaving the club by mutual consent and former player and manager Kenny Dalglish taking over.  In the 2011–12 season, Liverpool secured a record 8th League Cup success and reached the FA Cup final, but finished in eighth position, the worst league finish in 18 years; this led to the sacking of Dalglish.  He was replaced by Brendan Rodgers, whose Liverpool team in the 2013–14 season mounted an unexpected title charge to finish second behind champions Manchester City and subsequently return to the Champions League, scoring 101 goals in the process, the most since the 106 scored in the 1895–96 season.  Following a disappointing 2014–15 season, where Liverpool finished sixth in the league, and a poor start to the following campaign, Rodgers was sacked in October 2015. Rodgers was replaced by Jürgen Klopp.  Liverpool reached the finals of the Football League Cup and UEFA Europa League in Klopp's first season, finishing as runner-up in both competitions.  The club finished second in the 2018–19 season with 97 points (surpassing the 86 points gained during the 2008–09 season), losing only one game: a points record for a non-title winning side.  Klopp took Liverpool to successive Champions League finals in 2018 and 2019, with the club defeating Tottenham Hotspur 2–0 to win the 2019 UEFA Champions League Final.  Liverpool beat Flamengo of Brazil in the final 1–0 to win the FIFA Club World Cup for the first time.  Liverpool then went on to win the 2019–20 Premier League, winning their first top-flight league title in thirty years.  The club set multiple records in the season, including winning the league with seven games remaining making it the earliest any team has ever won the title, amassing a club record 99 points, and achieving a joint-record 32 wins in a top-flight season.  In January 2024, Jürgen Klopp announced that he would leave the club by the end of the season.  Colours and badge For much of Liverpool's history, its home colours have been all red.  When the club was founded in 1892, blue and white quartered shirts were used until the club adopted the city's colour of red in 1896.  The city's symbol of the liver bird was adopted as the club's badge (or crest, as it is sometimes known) in 1901, although it was not incorporated into the kit until 1955.  Liverpool continued to wear red shirts and white shorts until 1964 when manager Bill Shankly decided to change to an all-red strip.  Liverpool played in all red for the first time against Anderlecht, as Ian St John recalled in his autobiography: He [Shankly] thought the colour scheme would carry psychological impact – red for danger, red for power.  He came into the dressing room one day and threw a pair of red shorts to Ronnie Yeats.  \"Get into those shorts and let's see how you look\", he said.  \"Christ, Ronnie, you look awesome, terrifying.  You look 7 ft tall. \" \"Why not go the whole hog, boss?\" I suggested.  \"Why not wear red socks? Let's go out all in red. \" Shankly approved and an iconic kit was born. The Liverpool away strip has more often than not been all yellow or white shirts and black shorts, but there have been several exceptions.  An all grey kit was introduced in 1987, which was used until the 1991–92 centenary season when it was replaced by a combination of green shirts and white shorts.  After various colour combinations in the 1990s, including gold and navy, bright yellow, black and grey, and ecru, the club alternated between yellow and white away kits until the 2008–09 season, when it re-introduced the grey kit.  A third kit is designed for European away matches, though it is also worn in domestic away matches on occasions when the current away kit clashes with a team's home kit.  Between 2012 and 2015, the kits were designed by Warrior Sports, who became the club's kit providers at the start of the 2012–13 season.  In February 2015, Warrior's parent company New Balance announced it would be entering the global football market, with teams sponsored by Warrior now being outfitted by New Balance.  The only other branded shirts worn by the club were made by Umbro until 1985, when they were replaced by Adidas, who produced the kits until 1996 when Reebok took over.  They produced the kits for 10 years before Adidas made the kits from 2006 to 2012.  Nike became the club's official kit supplier at the start of the 2020–21 season.  Liverpool was the first English professional club to have a sponsor's logo on its shirts, after agreeing a deal with Hitachi in 1979.  However for the first few years of the deal, broadcasting rules meant that sponsors logos could not be shown on shirts for televised matches. Since then the club has been sponsored by Crown Paints, Candy, Carlsberg and Standard Chartered.  The contract with Carlsberg, which was signed in 1992, was the longest-lasting agreement in English top-flight football.  The association with Carlsberg ended at the start of the 2010–11 season, when Standard Chartered Bank became the club's sponsor. The Liverpool badge is based on the city's liver bird symbol, which in the past had been placed inside a shield.  In 1977, a red liver bird standing on a football (blazoned as \"Statant upon a football a Liver Bird wings elevated and addorsed holding in the beak a piece of seaweed gules\") was granted as a heraldic badge by the College of Arms to the English Football League intended for use by Liverpool.  However, Liverpool never made use of this badge.  In 1992, to commemorate the centennial of the club, a new badge was commissioned, including a representation of the Shankly Gates.  The next year twin flames were added at either side, symbolic of the Hillsborough memorial outside Anfield, where an eternal flame burns in memory of those who died in the Hillsborough disaster.  In 2012, Warrior Sports' first Liverpool kit removed the shield and gates, returning the badge to what had adorned Liverpool shirts in the 1970s; the flames were moved to the back collar of the shirt, surrounding the number 96 for the number who died at Hillsborough.  Sponsorship Stadium Anfield was built in 1884 on land adjacent to Stanley Park.  Situated 2 miles (3 km) from Liverpool city centre, it was originally used by Everton before the club moved to Goodison Park after a dispute over rent with Anfield owner John Houlding.  Left with an empty ground, Houlding founded Liverpool in 1892 and the club has played at Anfield ever since.  The capacity of the stadium at the time was 20,000, although only 100 spectators attended Liverpool's first match at Anfield. The Kop was built in 1906 due to the high turnout for matches and was called the Oakfield Road Embankment initially.  Its first game was on 1 September 1906 when the home side beat Stoke City 1–0.  In 1906 the banked stand at one end of the ground was formally renamed the Spion Kop after a hill in KwaZulu-Natal.  The hill was the site of the Battle of Spion Kop in the Second Boer War, where over 300 men of the Lancashire Regiment died, many of them from Liverpool.  At its peak, the stand could hold 28,000 spectators and was one of the largest single-tier stands in the world.  Many stadiums in England had stands named after Spion Kop, but Anfield's was the largest of them at the time; it could hold more supporters than some entire football grounds. Anfield could accommodate more than 60,000 supporters at its peak and had a capacity of 55,000 until the 1990s, when, following recommendations from the Taylor Report, all clubs in the Premier League were obliged to convert to all-seater stadiums in time for the 1993–94 season, reducing its capacity to 45,276.  The findings of the report precipitated the redevelopment of the Kemlyn Road Stand, which was rebuilt in 1992, coinciding with the centenary of the club, and was known as the Centenary Stand until 2017 when it was renamed the Kenny Dalglish Stand.  An extra tier was added to the Anfield Road end in 1998, which further increased the capacity of the ground but gave rise to problems when it was opened.  A series of support poles and stanchions were inserted to give extra stability to the top tier of the stand after movement of the tier was reported at the start of the 1999–2000 season. Because of restrictions on expanding the capacity at Anfield, Liverpool announced plans to move to the proposed Stanley Park Stadium in May 2002.  Planning permission was granted in July 2004, and in September 2006, Liverpool City Council agreed to grant Liverpool a 999-year lease on the proposed site.  Following the takeover of the club by George Gillett and Tom Hicks in February 2007, the proposed stadium was redesigned.  The new design was approved by the Council in November 2007.  The stadium was scheduled to open in August 2011 and would hold 60,000 spectators, with HKS, Inc.  contracted to build the stadium.  Construction was halted in August 2008, as Gillett and Hicks had difficulty in financing the £300 million needed for the development.  In October 2012, BBC Sport reported that Fenway Sports Group, the new owners of Liverpool FC, had decided to redevelop their current home at Anfield stadium, rather than building a new stadium in Stanley Park.  As part of the redevelopment the capacity of Anfield was to increase from 45,276 to approximately 60,000 and would cost approximately £150m.  When construction was completed on the new Main stand the capacity of Anfield was increased to 54,074.  This £100 million expansion added a third tier to the stand.  This was all part of a £260 million project to improve the Anfield area.  Jürgen Klopp the manager at the time described the stand as \"impressive. \"In June 2021, it was reported that Liverpool Council had given planning permission for the club to renovate and expand the Anfield Road stand, boosting the capacity by around 7,000 and taking the overall capacity at Anfield to 61,000.  The expansion, which is estimated to cost £60m, was described as \"a huge milestone\" by managing director Andy Hughes, and would also see rail seating being trialled in the Kop for the 2021–22 Premier League season.  Support Liverpool is one of the best supported clubs in the world.  The club states that its worldwide fan base includes 300 officially recognised Supporters Clubs in 100 different countries.  Notable groups include Spirit of Shankly.  The club takes advantage of this support through its worldwide summer tours, which has included playing in front of 101,000 in Michigan, U. S. , and 95,000 in Melbourne, Australia.  Liverpool fans often refer to themselves as Kopites, a reference to the fans who once stood, and now sit, on the Kop at Anfield.  In 2008 a group of fans decided to form a splinter club, A. F. C.  Liverpool, to play matches for fans who had been priced out of watching Premier League football. The song \"You'll Never Walk Alone\", originally from the Rodgers and Hammerstein musical Carousel and later recorded by Liverpool musicians Gerry and the Pacemakers, is the club's anthem and has been sung by the Anfield crowd since the early 1960s.  It has since gained popularity among fans of other clubs around the world.  The song's title adorns the top of the Shankly Gates, which were unveiled on 2 August 1982 in memory of former manager Bill Shankly.  The \"You'll Never Walk Alone\" portion of the Shankly Gates is also reproduced on the club's badge.  The club's supporters have been involved in two stadium disasters.  The first was the 1985 Heysel Stadium disaster, in which 39 people, mostly Italians and Juventus supporters, were killed.  They were confined to a corner by Liverpool fans who had charged in their direction; the weight of the cornered fans caused a wall to collapse.  UEFA laid the blame for the incident solely on the Liverpool supporters, and banned all English clubs from European competition for five years.  Liverpool was banned for an additional year, preventing it from participating in the 1990–91 European Cup, even though it won the League in 1990.  Twenty-seven fans were arrested on suspicion of manslaughter and were extradited to Belgium in 1987 to face trial.  In 1989, after a five-month trial in Belgium, 14 Liverpool fans were given three-year sentences for involuntary manslaughter; half of the terms were suspended. The second disaster took place during an FA Cup semi-final between Liverpool and Nottingham Forest at Hillsborough Stadium, Sheffield, on 15 April 1989.  Ninety-seven Liverpool fans died as a consequence of overcrowding at the Leppings Lane end, in what became known as the Hillsborough disaster.  In the following days, The Sun's coverage of the event spread falsehoods, particularly an article entitled \"The Truth\" that  claimed that Liverpool fans had robbed the dead and had urinated on and attacked the police.  Subsequent investigations proved the allegations false, leading to a boycott of the newspaper by Liverpool fans across the city and elsewhere; many still refuse to buy The Sun 30 years later.  Many support organisations were set up in the wake of the disaster, such as the Hillsborough Justice Campaign, which represents bereaved families, survivors and supporters in their efforts to secure justice.  Rivalries Liverpool's longest-established rivalry is with fellow Liverpool team Everton, against whom they contest the Merseyside derby.  The rivalry stems from Liverpool's formation and the dispute with Everton officials and the then owners of Anfield.  The Merseyside derby is one of the few local derbies which do not enforce fan segregation, and hence has been known as the \"friendly derby\".  Since the mid-1980s, the rivalry has intensified both on and off the field and, since the inception of the Premier League in 1992, the Merseyside derby has had more players sent off than any other Premier League game.  It has been referred to as \"the most ill-disciplined and explosive fixture in the Premier League\".  In terms of support within the city, the number of Liverpool fans outweighs Everton supporters by a ratio of 2:1. Liverpool's rivalry with Manchester United stems from the cities' competition in the Industrial Revolution of the 19th century.  Connected by the world's first inter-city railway, by road Liverpool and Manchester are separated by approximately 30 miles (48 km) along the East Lancs Road.  Ranked the two biggest clubs in England by France Football magazine, Liverpool and Manchester United are the most successful English teams in both domestic and international competitions, and both clubs have a global fanbase.  Viewed as one of the biggest rivalries in world football, it is considered the most famous fixture in English football.  The two clubs alternated as champions between 1964 and 1967, and Manchester United became the first English team to win the European Cup in 1968, followed by Liverpool's four European Cup victories.  Despite the 39 league titles and nine European Cups between them the two rivals have rarely been successful at the same time – Liverpool's run of titles in the 1970s and 1980s coincided with Manchester United's 26-year title drought, and United's success in the Premier League-era likewise coincided with Liverpool's 30-year title drought, and the two clubs have finished first and second in the league only five times.  Such is the rivalry between the clubs they rarely do transfer business with each other.  The last player to be transferred between the two clubs was Phil Chisnall, who moved to Liverpool from Manchester United in 1964.  Ownership and finances As the owner of Anfield and founder of Liverpool, John Houlding was the club's first chairman, a position he held from its founding in 1892 until 1904.  John McKenna took over as chairman after Houlding's departure.  McKenna subsequently became President of the Football League.  The chairmanship changed hands many times before John Smith, whose father was a shareholder of the club, took up the role in 1973.  He oversaw the most successful period in Liverpool's history before stepping down in 1990.  His successor was Noel White who became chairman in 1990.  In August 1991 David Moores, whose family had owned the club for more than 50 years, became chairman.  His uncle John Moores was also a shareholder at Liverpool and was chairman of Everton from 1961 to 1973.  Moores owned 51 percent of the club, and in 2004 expressed his willingness to consider a bid for his shares in Liverpool. Moores eventually sold the club to American businessmen George Gillett and Tom Hicks on 6 February 2007.  The deal valued the club and its outstanding debts at £218. 9 million.  The pair paid £5,000 per share, or £174. 1m for the total shareholding and £44. 8m to cover the club's debts.  Disagreements between Gillett and Hicks, and the fans' lack of support for them, resulted in the pair looking to sell the club.  Martin Broughton was appointed chairman of the club on 16 April 2010 to oversee its sale.  In May 2010, accounts were released showing the holding company of the club to be £350m in debt (due to leveraged takeover) with losses of £55m, causing auditor KPMG to qualify its audit opinion.  The group's creditors, including the Royal Bank of Scotland, took Gillett and Hicks to court to force them to allow the board to proceed with the sale of the club, the major asset of the holding company.  A High Court judge, Mr Justice Floyd, ruled in favour of the creditors and paved the way for the sale of the club to Fenway Sports Group (formerly New England Sports Ventures), although Gillett and Hicks still had the option to appeal.  Liverpool was sold to Fenway Sports Group on 15 October 2010 for £300m. Liverpool has been described as a global brand; a 2010 report valued the club's trademarks and associated intellectual property at £141m, an increase of £5m on the previous year.  Liverpool was given a brand rating of AA (Very Strong).  In April 2010 business magazine Forbes ranked Liverpool as the sixth most valuable football team in the world, behind Manchester United, Real Madrid, Arsenal, Barcelona and Bayern Munich; they valued the club at $822m (£532m), excluding debt.  Accountants Deloitte ranked Liverpool eighth in the Deloitte Football Money League, which ranks the world's football clubs in terms of revenue.  Liverpool's income in the 2009–10 season was €225. 3m.  According to a 2018 report by Deloitte, the club had an annual revenue of €424. 2 million for the previous year, and Forbes valued the club at $1. 944 billion.  In 2018, annual revenue increased to €513. 7 million, and Forbes valued the club at $2. 183 billion.  In 2019 revenue increased to €604 million (£533 million) according to Deloitte, with the club breaching the half a billion pounds mark. In April 2020, the owners of the club came under fire from fans and the media for deciding to furlough all non-playing staff during the COVID-19 pandemic.  In response to this, the club made a U-turn on the decision and apologised for their initial decision.  In April 2021 Forbes valued the club at $4. 1 billion, a two-year increase of 88%, making it the world's fifth-most-valuable football club.  Based on the latest rankings by Forbes, as of May 2023, Liverpool is ranked as the fourth most valuable football club in the world, behind Real Madrid, Manchester United and Barcelona; they valued the club at $5. 29 billion, an increase of 19% from 2022.  Liverpool in the media Liverpool featured in the first edition of BBC's Match of the Day, which screened highlights of their match against Arsenal at Anfield on 22 August 1964.  The first football match to be televised in colour was between Liverpool and West Ham United, broadcast live in March 1967.  Liverpool fans featured in the Pink Floyd song \"Fearless\", in which they sang excerpts from \"You'll Never Walk Alone\".  To mark the club's appearance in the 1988 FA Cup Final, Liverpool released the \"Anfield Rap\", a song featuring John Barnes and other members of the squad. A docudrama on the Hillsborough disaster, written by Jimmy McGovern, was screened in 1996.  It featured Christopher Eccleston as Trevor Hicks, who lost two teenage daughters in the disaster, went on to campaign for safer stadiums and helped to form the Hillsborough Families Support Group.  Liverpool featured in the 2001 film The 51st State, in which ex-hitman Felix DeSouza (Robert Carlyle) is a keen supporter of the team and the last scene takes place at a match between Liverpool and Manchester United.  The club also featured in the 1984 children's television show Scully, about a young boy who tries to gain a trial with Liverpool.  In the Doctor Who episode \"The Halloween Apocalypse\", aired in October 2021, features The Doctor (played by Jodie Whittaker) exiting the TARDIS outside Anfield as she exclaims: \"Liverpool? Anfield! Klopp era, classic!\".  Players First-team squad As of 31 January 2024Note: Flags indicate national team as defined under FIFA eligibility rules.  Players may hold more than one non-FIFA nationality.  Out on loan Note: Flags indicate national team as defined under FIFA eligibility rules.  Players may hold more than one non-FIFA nationality.  Reserves and Academy Former players Player records Club captains Since the establishment of the club in 1892, 46 players have been club captain of Liverpool F. C.  Andrew Hannah became the first captain of the club after Liverpool separated from Everton and formed its own club.  Alex Raisbeck, who was club captain from 1899 to 1909, was the longest serving captain before being overtaken by Steven Gerrard who served 12 seasons as Liverpool captain starting from the 2003–04 season.  The present captain is Virgil van Dijk, who in the 2023–24 season replaced Jordan Henderson who moved to Al-Ettifaq.  Player of the season Club officials Honours Liverpool's first trophy was the Lancashire League, which it won in the club's first season.  In 1901, the club won its first League title, while the nineteenth and most recent was in 2020.  Its first success in the FA Cup was in 1965.  In terms of the number of trophies won, Liverpool's most successful decade was the 1980s, when the club won six League titles, two FA Cups, four League Cups, one Football League Super Cup, five Charity Shields (one shared) and two European Cups.  In 2020, Liverpool became the first English club to have won a League title in eight different decades. The club has accumulated more top-flight wins and points than any other English team.  Liverpool also has the highest average league finishing position (3. 3) for the 50-year period to 2015 and second-highest average league finishing position for the period 1900–1999 after Arsenal, with an average league placing of 8. 7. Liverpool is the most successful British club in international football with fourteen trophies, having won the European Cup/UEFA Champions League, UEFA's premier club competition, six times, an English record and only surpassed by Real Madrid and A. C.  Milan.  Liverpool's fifth European Cup win, in 2005, meant that the club was awarded the trophy permanently and was also awarded a multiple-winner badge.  Liverpool also hold the English record of three wins in the UEFA Cup, UEFA's secondary club competition.  Liverpool also hold the English record of four wins in the UEFA Super Cup.  In 2019, the club won the FIFA Club World Cup for the first time, and also became the first English club to win the international treble of Club World Cup, Champions League and UEFA Super Cup.  \n"
     ]
    }
   ],
   "source": [
    "# clean data\n",
    "cleaned_text = page.text.replace('\\n\\n', ' ')\n",
    "cleaned_text = cleaned_text.replace('\\n', ' ')\n",
    "cleaned_text = cleaned_text.replace('.  ', '.')\n",
    "cleaned_text = cleaned_text.replace('.', '. ')\n",
    "\n",
    "\n",
    "\n",
    "# delete some text that it not be a sentence\n",
    "# find the index of \"Minor titles\"\n",
    "start_index = cleaned_text.find(\"Minor titles\")\n",
    "\n",
    "# If \"Minor titles\" is found, slice the string to remove the content from that point onward\n",
    "if start_index != -1:\n",
    "    cleaned_text = cleaned_text[:start_index]\n",
    "\n",
    "    # Print the cleaned text\n",
    "    print(cleaned_text)\n",
    "else:\n",
    "    print(\"String 'Minor titles' not found.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task2 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\earth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import the neccessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torchtext, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# check that I have cpu for train or not\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# same pattern when I restart the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task2.1 Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step1: Tokenize the text dataset to sentence and split the dataset on train, test, and validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\earth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download pubkt which is pre-trained for tokenizing text into sentences\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences =nltk.sent_tokenize(cleaned_text)\n",
    "# sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in training set: 187\n",
      "Number of samples in validation set: 33\n",
      "Number of samples in test set: 39\n"
     ]
    }
   ],
   "source": [
    "# set the random seed for reproducibility\n",
    "random_seed = 42\n",
    "\n",
    "# split the data into training, testing, and validation sets\n",
    "train_data, test_data = train_test_split(sentences, test_size=0.15, random_state=random_seed)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=random_seed)\n",
    "\n",
    "# Print the sizes of the sets\n",
    "print(f\"Number of samples in training set: {len(train_data)}\")\n",
    "print(f\"Number of samples in validation set: {len(val_data)}\")\n",
    "print(f\"Number of samples in test set: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The last player to be transferred between the two clubs was Phil Chisnall, who moved to Liverpool from Manchester United in 1964.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step2: Tokenize the sentence on train, test, and validation dataset to tokens (individual) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize to tranform sentence to tokens\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "\n",
    "# create function to tokenize the text\n",
    "tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example)}  \n",
    "\n",
    "# map the function to each example in the list\n",
    "tokenized_train_data = list(map(lambda example: tokenize_data(example, tokenizer), train_data))\n",
    "tokenized_test_data = list(map(lambda example: tokenize_data(example, tokenizer), test_data))\n",
    "tokenized_val_data = list(map(lambda example: tokenize_data(example, tokenizer), val_data))\n",
    "# tokenized_train_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step3: Numericalizing <br>\n",
    "create list that keep all vocab and add `unk` and `eos` to that list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create nested list that is suitable format for build_vocab_from_iterator function \n",
    "tokenized_train_dataset = [entry['tokens'] for entry in tokenized_train_data]\n",
    "tokenized_test_dataset = [entry['tokens'] for entry in tokenized_test_data]\n",
    "tokenized_val_dataset = [entry['tokens'] for entry in tokenized_val_data]\n",
    "# tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_train_dataset)\n",
    "vocab.insert_token('<unk>', 0)\n",
    "vocab.insert_token('<eos>', 1) #for print next word\n",
    "vocab.set_default_index(vocab['<unk>']) #word that not in vocab tranfer to <unk>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the vocabulary to a file\n",
    "torch.save(vocab, 'vocab.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1211\n"
     ]
    }
   ],
   "source": [
    "# print the number of vocab in total\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<eos>', 'the', ',', '.', 'in', 'and', 'of', 'liverpool', 'to']\n"
     ]
    }
   ],
   "source": [
    "# print 10 vocabs\n",
    "print(vocab.get_itos()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step4: Prepare data by separate data to batch\n",
    "\n",
    "For example, I have 2 sentences which are \"We have to go now\" and \"we will go to supermarket\" . I assign batch size = 3, we will get three batches of data \"We have to go\", \"now `<eos>` we will\", \"go to supermarket `<eos>`\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for split the dataset on batch\n",
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []\n",
    "    for example in dataset:\n",
    "        if example:\n",
    "            tokens = example.append('<eos>') # add <eos> to the end of each sentence\n",
    "            tokens = [vocab[token] for token in example] # change each word to number\n",
    "            data.extend(tokens)\n",
    "    data = torch.LongTensor(data)\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    data = data[:num_batches * batch_size] # to make sure that every batch is equal\n",
    "    data = data.view(batch_size, num_batches) #reshape \n",
    "    return data #[batch size, seq len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_data = get_data(tokenized_train_dataset, vocab, batch_size)\n",
    "valid_data = get_data(tokenized_val_dataset, vocab, batch_size)\n",
    "test_data  = get_data(tokenized_test_dataset,  vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   5,  494,    3,  ...,   98,  110,  483],\n",
       "        [   4,    1,    5,  ...,  461,    4,    1],\n",
       "        [   8,   14,   16,  ...,   77,  918,   17],\n",
       "        ...,\n",
       "        [ 113,   12,  591,  ...,    2,  288,   18],\n",
       "        [   3,   43,  438,  ...,   17,    2,   10],\n",
       "        [   9, 1012,    6,  ...,  898,   17,  712]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 297])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task2.2 Model architecture and the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"LM.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Language Model assignment, I create the LSTMLanguageModel class for build model. it consists of embedding layer to tranform the input tokens/words into number vector, multiple stacked LSTM layers to learn long-range dependencied in sequential data, dropout layers to cut out some data to prevent overfitting, and linear layer to tranforms the output from LSTM layer into prediction of next word of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model \n",
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hid_dim    = hid_dim\n",
    "        self.emb_dim    = emb_dim\n",
    "        \n",
    "        self.embedding  = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm       = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout    = nn.Dropout(dropout_rate)\n",
    "        self.fc         = nn.Linear(hid_dim, vocab_size) # fc is the last layer for \n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    # function for assigning the initial weight of W_e, W_h\n",
    "    def init_weights(self):\n",
    "        init_range_emb = 0.1\n",
    "        init_range_other = 1/math.sqrt(self.hid_dim)\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.emb_dim,\n",
    "                self.hid_dim).uniform_(-init_range_other, init_range_other) #W_e\n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hid_dim,   \n",
    "                self.hid_dim).uniform_(-init_range_other, init_range_other) #W_h\n",
    "    \n",
    "    # reset hidden\n",
    "    def init_hidden(self, batch_size, device): \n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        return hidden, cell\n",
    "        \n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach() \n",
    "        cell   = cell.detach()\n",
    "        return hidden, cell\n",
    "        \n",
    "    def forward(self, src, hidden):\n",
    "        #src: [batch_size, seq len]\n",
    "        embedding = self.dropout(self.embedding(src)) #Liverpool is\n",
    "        #embedding: [batch-size, seq len, emb dim]\n",
    "        output, hidden = self.lstm(embedding, hidden)\n",
    "        #ouput: [batch size, seq len, hid dim]\n",
    "        #hidden: [num_layers * direction, seq len, hid_dim]\n",
    "        output = self.dropout(output)\n",
    "        prediction =self.fc(output)\n",
    "        #prediction: [batch_size, seq_len, vocab_size]\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the parameters\n",
    "vocab_size = len(vocab)\n",
    "emb_dim = 1024                \n",
    "hid_dim = 50                \n",
    "num_layers = 1               \n",
    "dropout_rate = 0.5             \n",
    "lr = 1e-3                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\earth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,517,025 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model      = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
    "optimizer  = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion  = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function is used for getting the input and output batch for training process\n",
    "def get_batch(data, seq_len, idx):\n",
    "    #data #[batch size, bunch of tokens]\n",
    "    src    = data[:, idx:idx+seq_len]                   \n",
    "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "    \n",
    "    #reset the hidden every epoch\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
    "        optimizer.zero_grad() #clear all gradient\n",
    "        \n",
    "        #hidden does not need to be in the computational graph for efficiency\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        #get the input and output batch\n",
    "        src, target = get_batch(data, seq_len, idx) \n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        #put it on LSTM model that I created and printthe prediction\n",
    "        prediction, hidden = model(src, hidden)               \n",
    "\n",
    "        #need to reshape because criterion expects pred to be 2d and target to be 1d\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        #clipping to make gradient smaller to prevent exploding gradient \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        #update the model parameter\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    # average the training loss\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluate the model with validation dataset\n",
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len] \n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size= src.shape[0]\n",
    "\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "    # average the validation loss\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 1142.536\n",
      "\tValid Perplexity: 901.910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 1032.475\n",
      "\tValid Perplexity: 809.273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 817.584\n",
      "\tValid Perplexity: 614.836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 544.780\n",
      "\tValid Perplexity: 449.423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 404.232\n",
      "\tValid Perplexity: 375.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 334.085\n",
      "\tValid Perplexity: 350.076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 304.907\n",
      "\tValid Perplexity: 343.530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 283.143\n",
      "\tValid Perplexity: 342.620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 266.303\n",
      "\tValid Perplexity: 340.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 256.834\n",
      "\tValid Perplexity: 334.361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 241.427\n",
      "\tValid Perplexity: 327.129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 223.366\n",
      "\tValid Perplexity: 319.450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 216.301\n",
      "\tValid Perplexity: 311.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 200.075\n",
      "\tValid Perplexity: 304.044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 190.555\n",
      "\tValid Perplexity: 297.324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 178.566\n",
      "\tValid Perplexity: 292.302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 168.387\n",
      "\tValid Perplexity: 286.887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 159.368\n",
      "\tValid Perplexity: 282.324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 149.912\n",
      "\tValid Perplexity: 279.102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 146.034\n",
      "\tValid Perplexity: 274.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 135.602\n",
      "\tValid Perplexity: 270.942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 130.962\n",
      "\tValid Perplexity: 266.916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 125.794\n",
      "\tValid Perplexity: 262.821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 119.092\n",
      "\tValid Perplexity: 260.614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 114.907\n",
      "\tValid Perplexity: 259.756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 110.593\n",
      "\tValid Perplexity: 257.258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 106.864\n",
      "\tValid Perplexity: 254.948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 101.389\n",
      "\tValid Perplexity: 252.320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 98.313\n",
      "\tValid Perplexity: 249.898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 94.306\n",
      "\tValid Perplexity: 249.145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 90.349\n",
      "\tValid Perplexity: 246.677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 86.481\n",
      "\tValid Perplexity: 246.584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 85.191\n",
      "\tValid Perplexity: 245.587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 84.559\n",
      "\tValid Perplexity: 244.960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 82.153\n",
      "\tValid Perplexity: 245.129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 81.158\n",
      "\tValid Perplexity: 245.129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 80.718\n",
      "\tValid Perplexity: 245.104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 79.249\n",
      "\tValid Perplexity: 245.013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 79.796\n",
      "\tValid Perplexity: 244.972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 79.440\n",
      "\tValid Perplexity: 244.943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 78.488\n",
      "\tValid Perplexity: 244.927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 79.227\n",
      "\tValid Perplexity: 244.921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 79.550\n",
      "\tValid Perplexity: 244.917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 79.859\n",
      "\tValid Perplexity: 244.914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 79.737\n",
      "\tValid Perplexity: 244.914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 78.715\n",
      "\tValid Perplexity: 244.913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 79.719\n",
      "\tValid Perplexity: 244.913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 79.447\n",
      "\tValid Perplexity: 244.913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 79.074\n",
      "\tValid Perplexity: 244.913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 78.796\n",
      "\tValid Perplexity: 244.913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# training\n",
    "n_epochs = 50\n",
    "seq_len  = 30 #<----decoding length\n",
    "clip    = 0.25\n",
    "\n",
    "# to reduce the learning rate when the loss is not improve\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_data, optimizer, criterion, \n",
    "                batch_size, seq_len, clip, device)\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
    "                seq_len, device)\n",
    "\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    # save the model if the validation loss of model is improve\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-val-lstm_lm.pt')\n",
    "\n",
    "    #print the train and validation Perplexity (lower, better)\n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 247.795\n"
     ]
    }
   ],
   "source": [
    "# test model with test dataset\n",
    "\n",
    "model.load_state_dict(torch.load('best-val-lstm_lm.pt',  map_location=device))\n",
    "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 Demo function before creating deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens] # tranforms word to number (index in vocabs)\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            \n",
    "            #prediction: [batch size, seq len, vocab size]\n",
    "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
    "            \n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
    "            \n",
    "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
    "                break\n",
    "\n",
    "            indices.append(prediction)\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\earth\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMLanguageModel(\n",
       "  (embedding): Embedding(1211, 1024)\n",
       "  (lstm): LSTM(1024, 50, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=50, out_features=1211, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load saved model\n",
    "loaded_model = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate)\n",
    "loaded_model.load_state_dict(torch.load('best-val-lstm_lm.pt'))\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "liverpool is the club .\n",
      "\n",
      "0.7\n",
      "liverpool is the club and suspicion of the club , and , and overcrowding owner of the club ' s and everton .\n",
      "\n",
      "0.75\n",
      "liverpool is the club and suspicion of the fifa club has , and overcrowding owner of the club ' s and everton .\n",
      "\n",
      "0.8\n",
      "liverpool is the club and suspicion of his men , reuben , and overcrowding featuring could george increase of colour and everton also multiple-winner league days , as the all he to\n",
      "\n",
      "1.0\n",
      "liverpool is the club also suspicion of his men , reuben , and overcrowding featuring could george rivalries addorsed colour and everton also multiple-winner league days , as on all he football\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Liverpool is'\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "# temperature = 1 # since we want the most make-sense sentence, temperature must highest which is 1\n",
    "\n",
    "#smaller the temperature, more diverse tokens but comes \n",
    "#with a tradeoff of less-make-sense sentence\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, loaded_model, tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In web interface, it need to show only one answer that is the sentence with highest temparature which is 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
